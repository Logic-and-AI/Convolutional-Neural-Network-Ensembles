# -*- coding: utf-8 -*-
"""Transformer Metamodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EO4OCYqLTIVqs02k1mqyK0kQlFQsMlXy
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import itertools
import math

class ResidualBlock(nn.Module):
    def __init__(self, in_height, in_width, kernel_size, kernel_size_2, in_channels, out_channels, batchnorm):
        super().__init__()
        self.in_height = in_height
        self.in_width = in_width
        self.kernel_size = kernel_size
        self.kernel_size_2 = kernel_size_2
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.batchnorm = batchnorm
        self.padding1=(self.kernel_size - 1)//2
        self.padding2=(self.kernel_size_2 -1)//2

        if self.batchnorm == 1:
            self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding1, dilation=1, bias=False)
            self.bn1 = nn.BatchNorm2d(self.out_channels)
            self.conv2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=self.kernel_size_2, stride=1, padding=self.padding2, dilation=1, bias=False)
            self.bn2 = nn.BatchNorm2d(self.out_channels)
        else:
            self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding1, dilation=1, bias=True)
            self.conv2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=self.kernel_size_2, stride=1, padding=self.padding2, dilation=1, bias=True)

        if self.in_channels != self.out_channels:
            #self.shortcut = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1, bias=False), nn.BatchNorm2d(self.out_channels))
            self.shortcut = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1, bias=True)
        else:
            self.shortcut = nn.Identity()

    def forward(self, x):
         identity = self.shortcut(x)
         if self.batchnorm == 1:
            out = F.relu(self.bn1(self.conv1(x)), inplace=True)
            out = self.bn2(self.conv2(out))
         else:
            out = F.relu(self.conv1(x), inplace=True)
            out = self.conv2(out)
         out += identity
         return F.relu(out, inplace=True)

    def output_shape(self):
        return self.in_height, self.in_width


class ConvolutionalBlock(nn.Module):
    def __init__(self, in_height, in_width, kernel_size, in_channels, out_channels, stride, padding, dilation, batchnorm):
        super().__init__()
        self.padding = padding
        self.dilation = dilation
        self.in_height = in_height
        self.in_width = in_width
        self.kernel_size = kernel_size
        self.stride = stride
        self.batchnorm = batchnorm

        if self.batchnorm == 1:
            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation, bias=False)
            self.bn1 = nn.BatchNorm2d(out_channels)
        else:
            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation, bias=True)


    def forward(self, x):
      if self.batchnorm == 1:
        out = F.relu(self.bn1(self.conv1(x)), inplace=True)
      else:
        out = F.relu(self.conv1(x))
      return out


    def output_shape(self):
        h1 = ((self.in_height+2*self.padding-self.dilation*(self.kernel_size-1)-1)//self.stride)+1
        w1 = ((self.in_width+2*self.padding-self.dilation*(self.kernel_size-1)-1)//self.stride)+1
        return h1, w1

class DenseBlock(nn.Module):
    def __init__(self, in_height, in_width, kernel_size, batchnorm, in_channels=16, growth_rate=16, num_layers=8): # signature=[in_channels, growth_rate, num_layers]
        super().__init__()
        self.layers = nn.ModuleList()
        self.in_height = in_height
        self.in_width = in_width
        self.kernel_size = kernel_size
        self.padding = (self.kernel_size - 1) // 2
        self.batchnorm = batchnorm
        self.in_channels = in_channels
        self.channels = in_channels
        self.growth_rate = growth_rate
        self.num_layers = num_layers
        self.out_channels = self.in_channels + self.growth_rate * self.num_layers

        if self.batchnorm:
          for i in range(self.num_layers):
            layer = nn.Sequential(
                nn.BatchNorm2d(self.channels),
                nn.ReLU(inplace=True),
                nn.Conv2d(self.channels, self.growth_rate, kernel_size=self.kernel_size, padding=self.padding, stride=1, dilation=1, bias=False)
            )
            self.layers.append(layer)
            self.channels += self.growth_rate
        else:
          for i in range(self.num_layers):
            layer = nn.Sequential(
                nn.ReLU(inplace=True),
                nn.Conv2d(self.channels, self.growth_rate, kernel_size=self.kernel_size, padding=self.padding, stride=1, dilation=1, bias=True)
            )
            self.layers.append(layer)
            self.channels += self.growth_rate


    def forward(self, x):
        features = [x]
        for layer in self.layers:
            out = layer(torch.cat(features, 1))
            features.append(out)
        return torch.cat(features, 1)

    def output_shape(self):
        return self.in_height, self.in_width

    def output_channels(self):
        return self.out_channels



#block specs res block:    [kernel_size, kernel_size_2, in_channels, out_channels, batchnorm, 'res']
#block specs conv block:   [kernel_size, in_channels, out_channels, stride, padding, dilation, batchnorm, 'conv']
#block specs dense block:  [kernel_size, batchnorm, in_channels=16, growth_rate=16, num_layers=8, 'dense']

# Positional Encoding Module

class TwoDPositionalEncoding(nn.Module):
    def __init__(self, embed_dim: int, height: int, width: int):
        super().__init__()
        self.row_embed = nn.Parameter(torch.randn(height, embed_dim))
        self.col_embed = nn.Parameter(torch.randn(width,  embed_dim))

    def forward(self, H: int, W: int) -> torch.Tensor:
        pos = self.row_embed[:H].unsqueeze(1) + self.col_embed[:W].unsqueeze(0)
        return pos.flatten(0, 1)




class CNN(nn.Module):
    def __init__(self, input_dim, img_height, img_width, block_specs,
                 pooling_kernel=[2, 2], pooling_stride=[2, 2], pooling_padding=[0, 0],
                 fc_layers=[10], fc_dropout=0.0, token_dim=None):
        super().__init__()
        self.conv_depth = len(block_specs)
        self.num_fc_layers = len(fc_layers)
        self.blocks = nn.ModuleList()
        self.fc_blocks = nn.ModuleList()
        self.fc_dropout = fc_dropout
        self.token_dim = token_dim

        current_channels = input_dim
        current_in_height = img_height
        current_in_width = img_width

        if len(block_specs) == len(pooling_kernel) == len(pooling_stride) == len(pooling_padding):
            for i in range(self.conv_depth):
                if block_specs[i][-1] == 'res':
                    block = ResidualBlock(current_in_height, current_in_width, *block_specs[i][0:5])
                    self.blocks.append(block)
                    current_in_height, current_in_width = block.output_shape()
                    if pooling_kernel[i] > 0:
                        self.blocks.append(nn.MaxPool2d(kernel_size=pooling_kernel[i], stride=pooling_stride[i], padding=pooling_padding[i]))
                        current_in_width = ((current_in_width + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                        current_in_height = ((current_in_height + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                    current_channels = block_specs[i][3]
                elif block_specs[i][-1] == 'dense':
                    block = DenseBlock(current_in_height, current_in_width, *block_specs[i][0:5])
                    self.blocks.append(block)
                    current_in_height, current_in_width = block.output_shape()
                    if pooling_kernel[i] > 0:
                        self.blocks.append(nn.MaxPool2d(kernel_size=pooling_kernel[i], stride=pooling_stride[i], padding=pooling_padding[i]))
                        current_in_width = ((current_in_width + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                        current_in_height = ((current_in_height + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                    current_channels = block.output_channels()
                else:
                    block = ConvolutionalBlock(current_in_height, current_in_width, *block_specs[i][0:7])
                    self.blocks.append(block)
                    current_in_height, current_in_width = block.output_shape()
                    if pooling_kernel[i] > 0:
                        self.blocks.append(nn.MaxPool2d(kernel_size=pooling_kernel[i], stride=pooling_stride[i], padding=pooling_padding[i]))
                        current_in_width = ((current_in_width + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                        current_in_height = ((current_in_height + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                    current_channels = block_specs[i][2]

            self.final_height, self.final_width = current_in_height, current_in_width
            self.final_channels = current_channels
            self.flattened_dim = self.final_height * self.final_width * self.final_channels
            self.spatial_tokens = self.final_height * self.final_width

            if token_dim is None:
                for i in range(self.num_fc_layers - 1):
                    self.fc_blocks.append(nn.Linear(self.flattened_dim, fc_layers[i]))
                    self.fc_blocks.append(nn.ReLU())
                    if self.fc_dropout > 0:
                        self.fc_blocks.append(nn.Dropout(p=self.fc_dropout))
                    self.flattened_dim = fc_layers[i]
                self.fc_blocks.append(nn.Linear(self.flattened_dim, fc_layers[-1]))
                self.fc_block_sequence = nn.Sequential(*self.fc_blocks)
            else:
                self.token_projection = nn.Linear(self.final_channels, token_dim)
                self.positional_encoding = TwoDPositionalEncoding(
                embed_dim = token_dim,
                height    = self.final_height,
                width     = self.final_width
                )

        else:
            raise ValueError("All supplied lists for the convolutional layers must be of the same length.")

        self.block_sequence = nn.Sequential(*self.blocks)

    def _forward_convs(self, x):
        return self.block_sequence(x)

    def _forward_fc(self, x):
        return self.fc_block_sequence(x)

    def forward(self, x):
        x = self._forward_convs(x)

        if self.token_dim is None:
            x = x.flatten(start_dim=1)
            return self._forward_fc(x)
        else:
            B, C, H, W = x.shape
            x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)
            x = self.token_projection(x)
            pos = self.positional_encoding(H, W)
            x = x + pos.unsqueeze(0)

            return x

    def pass_final_dimensions(self):
        return self.final_height, self.final_width, self.token_dim, self.final_channels


class Meta_model(nn.Module):
    def __init__(self, input_channels, img_height, img_width, base_model_specs, token_dim=None, encoder_layers=2, encoder_heads=1, meta_fc_layers=[10], meta_fc_dropout=0.0):
        super().__init__()

        self.token_dim = token_dim
        self.base_model_specs = base_model_specs
        self.num_base_models = len(base_model_specs)
        self.model_specs = nn.ModuleList()
        channel_cardinalities = []

        for model in base_model_specs:
            cnn = CNN(input_channels, img_height, img_width, block_specs=model[0],
                      pooling_kernel=model[1], pooling_stride=model[2], pooling_padding=model[3],
                      fc_layers=model[4], fc_dropout=model[5], token_dim=token_dim)
            self.model_specs.append(cnn)
            final_height, final_width, _, channels = cnn.pass_final_dimensions()
            if token_dim is None:
                channel_cardinalities.append(channels)
            else:
                tokens_per_pos = final_height * final_width
                channel_cardinalities.append(tokens_per_pos)

        if token_dim is not None:
            self.model_embedding = nn.Embedding(self.num_base_models, token_dim)
        else:
            self.model_embedding = None

        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=token_dim, nhead=encoder_heads, dim_feedforward=256, dropout=0.1, batch_first=True),
            num_layers=encoder_layers
        )

        self.meta_fc_blocks = nn.ModuleList()
        meta_input_dim = token_dim * sum(channel_cardinalities)
        for i in range(len(meta_fc_layers) - 1):
            self.meta_fc_blocks.append(nn.Linear(meta_input_dim, meta_fc_layers[i]))
            self.meta_fc_blocks.append(nn.ReLU())
            if meta_fc_dropout > 0:
                self.meta_fc_blocks.append(nn.Dropout(p=meta_fc_dropout))
            meta_input_dim = meta_fc_layers[i]
        self.meta_fc_blocks.append(nn.Linear(meta_input_dim, meta_fc_layers[-1]))
        self.meta_block_sequence = nn.Sequential(*self.meta_fc_blocks)

    def forward(self, x):
        features = []
        for i, model in enumerate(self.model_specs):
            token = model(x)
            embed = self.model_embedding.weight[i]
            token = token + embed[None, None, :]
            features.append(token)

        tokens = torch.cat(features, dim=1)
        tokens = self.transformer(tokens)
        x_stack = tokens.flatten(start_dim=1)
        return self.meta_block_sequence(x_stack)