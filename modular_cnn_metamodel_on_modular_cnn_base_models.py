# -*- coding: utf-8 -*-
"""Modular CNN metamodel on modular CNN base models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dj5CmTQyA4vcmtFCSSSaUU8rDPpIZn5c
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import itertools
import math

class ResidualBlock(nn.Module):
    def __init__(self, in_height, in_width, kernel_size, kernel_size_2, in_channels, out_channels, batchnorm):
        super().__init__()
        self.in_height = in_height
        self.in_width = in_width
        self.kernel_size = kernel_size
        self.kernel_size_2 = kernel_size_2
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.batchnorm = batchnorm
        self.padding1=(self.kernel_size - 1)//2
        self.padding2=(self.kernel_size_2 -1)//2

        if self.batchnorm == 1:
            self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding1, dilation=1, bias=False)
            self.bn1 = nn.BatchNorm2d(self.out_channels)
            self.conv2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=self.kernel_size_2, stride=1, padding=self.padding2, dilation=1, bias=False)
            self.bn2 = nn.BatchNorm2d(self.out_channels)
        else:
            self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding1, dilation=1, bias=True)
            self.conv2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=self.kernel_size_2, stride=1, padding=self.padding2, dilation=1, bias=True)

        if self.in_channels != self.out_channels:
            #self.shortcut = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1, bias=False), nn.BatchNorm2d(self.out_channels))
            self.shortcut = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1, bias=True)
        else:
            self.shortcut = nn.Identity()

    def forward(self, x):
         identity = self.shortcut(x)
         if self.batchnorm == 1:
            out = F.relu(self.bn1(self.conv1(x)), inplace=True)
            out = self.bn2(self.conv2(out))
         else:
            out = F.relu(self.conv1(x), inplace=True)
            out = self.conv2(out)
         out += identity
         return F.relu(out, inplace=True)

    def output_shape(self):
        return self.in_height, self.in_width


class ConvolutionalBlock(nn.Module):
    def __init__(self, in_height, in_width, kernel_size, in_channels, out_channels, stride, padding, dilation, batchnorm):
        super().__init__()
        self.padding = padding
        self.dilation = dilation
        self.in_height = in_height
        self.in_width = in_width
        self.kernel_size = kernel_size
        self.stride = stride
        self.batchnorm = batchnorm

        if self.batchnorm == 1:
            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation, bias=False)
            self.bn1 = nn.BatchNorm2d(out_channels)
        else:
            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation, bias=True)


    def forward(self, x):
      if self.batchnorm == 1:
        out = F.relu(self.bn1(self.conv1(x)), inplace=True)
      else:
        out = F.relu(self.conv1(x))
      return out


    def output_shape(self):
        h1 = ((self.in_height+2*self.padding-self.dilation*(self.kernel_size-1)-1)//self.stride)+1
        w1 = ((self.in_width+2*self.padding-self.dilation*(self.kernel_size-1)-1)//self.stride)+1
        return h1, w1

class DenseBlock(nn.Module):
    def __init__(self, in_height, in_width, kernel_size, batchnorm, in_channels=16, growth_rate=16, num_layers=8): # signature=[in_channels, growth_rate, num_layers]
        super().__init__()
        self.layers = nn.ModuleList()
        self.in_height = in_height
        self.in_width = in_width
        self.kernel_size = kernel_size
        self.padding = (self.kernel_size - 1) // 2
        self.batchnorm = batchnorm
        self.in_channels = in_channels
        self.channels = in_channels
        self.growth_rate = growth_rate
        self.num_layers = num_layers
        self.out_channels = self.in_channels + self.growth_rate * self.num_layers

        if self.batchnorm:
          for i in range(self.num_layers):
            layer = nn.Sequential(
                nn.BatchNorm2d(self.channels),
                nn.ReLU(inplace=True),
                nn.Conv2d(self.channels, self.growth_rate, kernel_size=self.kernel_size, padding=self.padding, stride=1, dilation=1, bias=False)
            )
            self.layers.append(layer)
            self.channels += self.growth_rate
        else:
          for i in range(self.num_layers):
            layer = nn.Sequential(
                nn.ReLU(inplace=True),
                nn.Conv2d(self.channels, self.growth_rate, kernel_size=self.kernel_size, padding=self.padding, stride=1, dilation=1, bias=True)
            )
            self.layers.append(layer)
            self.channels += self.growth_rate


    def forward(self, x):
        features = [x]
        for layer in self.layers:
            out = layer(torch.cat(features, 1))
            features.append(out)
        return torch.cat(features, 1)

    def output_shape(self):
        return self.in_height, self.in_width

    def output_channels(self):
        return self.out_channels


class ResidualBlock1D(nn.Module):
    def __init__(self, in_height, in_width, kernel_size, kernel_size_2, in_channels, out_channels, batchnorm):
        super().__init__()
        self.in_height = in_height
        self.in_width = in_width
        self.kernel_size = kernel_size
        self.kernel_size_2 = kernel_size_2
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.batchnorm = batchnorm
        self.padding1=(self.kernel_size - 1)//2
        self.padding2=(self.kernel_size_2 -1)//2

        if self.batchnorm == 1:
            self.conv1 = nn.Conv1d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding1, dilation=1, bias=False)
            self.bn1 = nn.BatchNorm1d(self.out_channels)
            self.conv2 = nn.Conv1d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=self.kernel_size_2, stride=1, padding=self.padding2, dilation=1, bias=False)
            self.bn2 = nn.BatchNorm1d(self.out_channels)
        else:
            self.conv1 = nn.Conv1d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=1, padding=self.padding1, dilation=1, bias=True)
            self.conv2 = nn.Conv1d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=self.kernel_size_2, stride=1, padding=self.padding2, dilation=1, bias=True)

        if self.in_channels != self.out_channels:
            #self.shortcut = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1, bias=False), nn.BatchNorm2d(self.out_channels))
            self.shortcut = nn.Conv1d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1, bias=True)
        else:
            self.shortcut = nn.Identity()

    def forward(self, x):
         identity = self.shortcut(x)
         if self.batchnorm == 1:
            out = F.relu(self.bn1(self.conv1(x)), inplace=True)
            out = self.bn2(self.conv2(out))
         else:
            out = F.relu(self.conv1(x), inplace=True)
            out = self.conv2(out)
         out += identity
         return F.relu(out, inplace=True)

    def output_shape(self):
        return self.in_width


class DenseBlock1D(nn.Module):
    def __init__(self, in_height, in_width, kernel_size, batchnorm, in_channels=16, growth_rate=16, num_layers=8):
        super().__init__()
        self.layers = nn.ModuleList()
        self.in_height = in_height
        self.in_width = in_width
        self.kernel_size = kernel_size
        self.padding = (self.kernel_size - 1) // 2
        self.batchnorm = batchnorm
        self.in_channels = in_channels
        self.channels = in_channels
        self.growth_rate = growth_rate
        self.num_layers = num_layers
        self.out_channels = self.in_channels + self.growth_rate * self.num_layers

        if self.batchnorm:
          for i in range(self.num_layers):
            layer = nn.Sequential(
                nn.BatchNorm1d(self.channels),
                nn.ReLU(inplace=True),
                nn.Conv1d(self.channels, self.growth_rate, kernel_size=self.kernel_size, padding=self.padding, stride=1, dilation=1, bias=False)
            )
            self.layers.append(layer)
            self.channels += self.growth_rate
        else:
          for i in range(self.num_layers):
            layer = nn.Sequential(
                nn.ReLU(inplace=True),
                nn.Conv1d(self.channels, self.growth_rate, kernel_size=self.kernel_size, padding=self.padding, stride=1, dilation=1, bias=True)
            )
            self.layers.append(layer)
            self.channels += self.growth_rate


    def forward(self, x):
        features = [x]
        for layer in self.layers:
            out = layer(torch.cat(features, 1))
            features.append(out)
        return torch.cat(features, 1)

    def output_shape(self):
        return self.in_width

    def output_channels(self):
        return self.out_channels



class ConvolutionalBlock1D(nn.Module):
    def __init__(self, in_height, in_width, kernel_size, in_channels, out_channels, stride, padding, dilation, batchnorm):
        super().__init__()
        self.padding = padding
        self.dilation = dilation
        self.in_height = in_height
        self.in_width = in_width
        self.kernel_size = kernel_size
        self.stride = stride
        self.batchnorm = batchnorm

        if self.batchnorm == 1:
            self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation, bias=False)
            self.bn1 = nn.BatchNorm1d(out_channels)
        else:
            self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation, bias=True)


    def forward(self, x):
      if self.batchnorm == 1:
        out = F.relu(self.bn1(self.conv1(x)), inplace=True)
      else:
        out = F.relu(self.conv1(x))
      return out


    def output_shape(self):
        w1 = ((self.in_width+2*self.padding-self.dilation*(self.kernel_size-1)-1)//self.stride)+1
        return w1

#block specs res block:    [kernel_size, kernel_size_2, in_channels, out_channels, batchnorm, 'res']
#block specs conv block:   [kernel_size, in_channels, out_channels, stride, padding, dilation, batchnorm, 'conv']
#block specs dense block:  [kernel_size, batchnorm, in_channels=16, growth_rate=16, num_layers=8, 'dense']


class CNN(nn.Module):
    def __init__(self, input_dim, img_height, img_width, block_specs,
                 pooling_kernel=[2, 2], pooling_stride=[2, 2], pooling_padding=[0, 0],
                 fc_layers=[10], fc_dropout=0.0, token_dim=None, conv1d=False):
        super().__init__()
        self.conv_depth = len(block_specs)
        self.num_fc_layers = len(fc_layers)
        self.blocks = nn.ModuleList()
        self.fc_blocks = nn.ModuleList()
        self.fc_dropout = fc_dropout
        self.token_dim = token_dim
        self.conv1d = conv1d

        current_channels = input_dim
        current_in_height = img_height
        current_in_width = img_width

        if len(block_specs) == len(pooling_kernel) == len(pooling_stride) == len(pooling_padding):
            for i in range(self.conv_depth):
                if block_specs[i][-1] == 'res':
                    if conv1d:
                        block = ResidualBlock1D(1, current_in_width, *block_specs[i][0:5])
                    else:
                        block = ResidualBlock(current_in_height, current_in_width, *block_specs[i][0:5])
                    self.blocks.append(block)
                    if not conv1d:
                        current_in_height, current_in_width = block.output_shape()
                        if pooling_kernel[i] > 0:
                            self.blocks.append(nn.MaxPool2d(kernel_size=pooling_kernel[i], stride=pooling_stride[i], padding=pooling_padding[i]))
                            current_in_width = ((current_in_width + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                            current_in_height = ((current_in_height + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                    else:
                        current_in_width = block.output_shape()
                        if pooling_kernel[i] > 0:
                            self.blocks.append(nn.MaxPool1d(kernel_size=pooling_kernel[i], stride=pooling_stride[i], padding=pooling_padding[i]))
                            current_in_width = ((current_in_width + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                    current_channels = block_specs[i][3]

                elif block_specs[i][-1] == 'dense':
                    if conv1d:
                        block = DenseBlock1D(1, current_in_width, *block_specs[i][0:5])
                    else:
                        block = DenseBlock(current_in_height, current_in_width, *block_specs[i][0:5])
                    self.blocks.append(block)
                    if not conv1d:
                        current_in_height, current_in_width = block.output_shape()
                        if pooling_kernel[i] > 0:
                            self.blocks.append(nn.MaxPool2d(kernel_size=pooling_kernel[i], stride=pooling_stride[i], padding=pooling_padding[i]))
                            current_in_width = ((current_in_width + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                            current_in_height = ((current_in_height + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                    else:
                        current_in_width = block.output_shape()
                        if pooling_kernel[i] > 0:
                            self.blocks.append(nn.MaxPool1d(kernel_size=pooling_kernel[i], stride=pooling_stride[i], padding=pooling_padding[i]))
                            current_in_width = ((current_in_width + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                    current_channels = block.out_channels

                else:
                    if conv1d:
                        block = ConvolutionalBlock1D(1, current_in_width, *block_specs[i][0:7])
                    else:
                        block = ConvolutionalBlock(current_in_height, current_in_width, *block_specs[i][0:7])
                    self.blocks.append(block)
                    if not conv1d:
                        current_in_height, current_in_width = block.output_shape()
                        if pooling_kernel[i] > 0:
                            self.blocks.append(nn.MaxPool2d(kernel_size=pooling_kernel[i], stride=pooling_stride[i], padding=pooling_padding[i]))
                            current_in_width = ((current_in_width + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                            current_in_height = ((current_in_height + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                    else:
                        current_in_width = block.output_shape()
                        if pooling_kernel[i] > 0:
                            self.blocks.append(nn.MaxPool1d(kernel_size=pooling_kernel[i], stride=pooling_stride[i], padding=pooling_padding[i]))
                            current_in_width = ((current_in_width + 2 * pooling_padding[i] - pooling_kernel[i]) // pooling_stride[i]) + 1
                    current_channels = block_specs[i][2]

            if not conv1d:
                self.final_height, self.final_width = current_in_height, current_in_width
                self.final_channels = current_channels
                self.flattened_dim = self.final_height * self.final_width * self.final_channels
                self.spatial_dim = self.final_height * self.final_width
            else:
                self.final_width = current_in_width
                self.final_channels = current_channels
                self.flattened_dim = self.final_channels * self.final_width
                self.spatial_dim = self.final_width

            if token_dim is None:
                for i in range(self.num_fc_layers - 1):
                    self.fc_blocks.append(nn.Linear(self.flattened_dim if i == 0 else fc_layers[i - 1], fc_layers[i]))
                    self.fc_blocks.append(nn.ReLU())
                    if self.fc_dropout > 0:
                        self.fc_blocks.append(nn.Dropout(p=self.fc_dropout))
                self.fc_blocks.append(nn.Linear(fc_layers[-2] if len(fc_layers) > 1 else self.flattened_dim, fc_layers[-1]))
                self.fc_block_sequence = nn.Sequential(*self.fc_blocks)
            else:
                if not conv1d:
                    self.token_projection = nn.Linear(self.spatial_dim, token_dim)
                else:
                    self.token_projection = nn.Identity()
        else:
            raise ValueError("All supplied lists for the convolutional layers must be of the same length.")

        self.block_sequence = nn.Sequential(*self.blocks)

    def _forward_convs(self, x):
        return self.block_sequence(x)

    def _forward_fc(self, x):
        return self.fc_block_sequence(x)

    def forward(self, x):
        x = self._forward_convs(x)

        if self.token_dim is None:
            x = x.flatten(start_dim=1)
            return self._forward_fc(x)
        else:
            if self.conv1d:
                return x
            else:
                B, C, H, W = x.shape
                x = x.view(B, C, H * W)
                x = self.token_projection(x)
                return x

    def pass_final_dimensions(self):
        if self.conv1d:
            return 1, 1, self.token_dim if self.token_dim else 0, self.final_channels
        else:
            return self.final_height, self.final_width, self.token_dim if self.token_dim else self.flattened_dim, self.final_channels




class Meta_model(nn.Module):
    def __init__(self, input_channels, img_height, img_width, base_model_specs,
                 token_dim=None, meta_block_specs=None, meta_pooling_kernel=[2], meta_pooling_stride=[2], meta_pooling_padding=[0],
                 meta_fc_layers=[10], meta_dropout=0.0):
        super().__init__()

        self.token_dim = token_dim
        self.base_model_specs = base_model_specs
        self.meta_block_specs = meta_block_specs
        self.num_base_models = len(base_model_specs)
        self.model_specs = nn.ModuleList()
        self.meta_fc_layers = meta_fc_layers
        channel_cardinalities = []

        for model in base_model_specs:
            cnn = CNN(input_channels, img_height, img_width, block_specs=model[0],
                      pooling_kernel=model[1], pooling_stride=model[2], pooling_padding=model[3],
                      fc_layers=model[4], fc_dropout=model[5], token_dim=token_dim)
            self.model_specs.append(cnn)
            _, _, _, channels = cnn.pass_final_dimensions()
            channel_cardinalities.append(channels)

        self.total_channels = sum(channel_cardinalities)

        meta_spatial_dim = token_dim
        meta_flattened_dim = self.total_channels * meta_spatial_dim
        self.meta_fc_layers.insert(0, meta_flattened_dim)
        self.final_base_channels=self.total_channels

        if self.meta_block_specs[0][-1]=='conv':
          self.meta_block_specs[0][1]=self.final_base_channels
        else:
          self.meta_block_specs[0][2]=self.final_base_channels


        # CNN class meta model:
        self.meta_cnn = CNN(
            input_dim=self.total_channels,
            img_height=1,
            img_width=token_dim,
            block_specs=meta_block_specs,
            pooling_kernel=meta_pooling_kernel,
            pooling_stride=meta_pooling_stride,
            pooling_padding=meta_pooling_padding,
            fc_layers=self.meta_fc_layers,
            fc_dropout=meta_dropout,
            token_dim=None,
            conv1d=True,
                    )

    def forward(self, x):
        features = []
        for model in self.model_specs:
            token = model(x)
            features.append(token)
        x_stack = torch.cat(features, dim=1)
        return self.meta_cnn(x_stack)