# -*- coding: utf-8 -*-
"""Dilated Convolutions on concatenated feature maps metamodel

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TAxMHHyR9h4ApDfAxOnusjDfb4L8Ctpx
"""

class Meta_model(nn.Module):
    def __init__(self, input_channels, img_height, img_width, base_model_specs, token_dim=None, meta_channels=2, meta_fc_layers=[10], meta_dropout=0.0):
        super().__init__()

        self.token_dim = token_dim
        self.base_model_specs = base_model_specs
        self.num_base_models = len(base_model_specs)
        self.model_specs = nn.ModuleList()
        self.meta_channels = meta_channels

        self.meta_dropout=meta_dropout
        channel_cardinalities = []

        for model in base_model_specs:
            cnn = CNN(input_channels, img_height, img_width, block_specs=model[0],
                      pooling_kernel=model[1], pooling_stride=model[2], pooling_padding=model[3],
                      fc_layers=model[4], fc_dropout=model[5], token_dim=token_dim)
            self.model_specs.append(cnn)
            _, _, _, channels = cnn.pass_final_dimensions()
            channel_cardinalities.append(channels)

        self.num_base_models = sum(channel_cardinalities)

        self.conv = nn.Conv1d(
            in_channels=1,
            out_channels=self.meta_channels,
            kernel_size=self.num_base_models,
            dilation=self.token_dim,
            padding=0
        )

        in_length=self.num_base_models * self.token_dim
        self.conv_layer_length = (in_length - self.token_dim * (self.num_base_models - 1))
        self.flat_dim = self.meta_channels * self.conv_layer_length
        self.meta_fc_blocks = nn.ModuleList()
        meta_input_dim = self.flat_dim
        for i in range(len(meta_fc_layers) - 1):
            self.meta_fc_blocks.append(nn.Linear(meta_input_dim, meta_fc_layers[i]))
            self.meta_fc_blocks.append(nn.ReLU())
            if meta_dropout > 0:
                self.meta_fc_blocks.append(nn.Dropout(p=meta_dropout))
            meta_input_dim = meta_fc_layers[i]
        self.meta_fc_blocks.append(nn.Linear(meta_input_dim, meta_fc_layers[-1]))
        self.meta_block_sequence = nn.Sequential(*self.meta_fc_blocks)

    def forward(self, x):
      seq_chunks = []
      for cnn in self.model_specs:
          token = cnn(x)
          seq_chunks.append(token.flatten(start_dim=1))
      x_seq = torch.cat(seq_chunks, dim=1)
      x_seq = x_seq.unsqueeze(1)
      x_seq = F.relu(self.conv(x_seq))
      x_seq = x_seq.flatten(start_dim=1)
      return self.meta_block_sequence(x_seq)